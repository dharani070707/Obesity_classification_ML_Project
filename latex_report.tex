\documentclass[12pt,a4paper]{report}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{float}
\usepackage{lipsum}

\geometry{margin=1in}
\setstretch{1.25}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    pdftitle={Machine Learning Report},
}

\begin{document}

% ---------------------------------------------------
% Title Page
% ---------------------------------------------------
\begin{titlepage}
    \centering
    \vspace*{2cm}
    {\LARGE \textbf{Classification of Weight Categories}}\\[1.5cm]
    {\large \textbf{Course Project Report}}\\[0.5cm]
    {\large Department of Computer Science and Engineering}\\[0.2cm]
    {\large International Institute of Information Technology, Bangalore}\\[0.5cm]
    \vfill
    \textbf{Submitted by:}\\
    \textit{Dharani Prasad S}\\
    Roll No: MT2025043\\[0.2cm]
    \textit{Abhishek Prasanna}\\
    Roll No: MT2025007\\[0.2cm]
    
    \vfill
    \textbf{Course:} AIT-511 Machine Learning\\
    \vfill
    {\large \today}
\end{titlepage}

\tableofcontents
\newpage

% ---------------------------------------------------
\chapter{Abstract}
This project aims to develop a machine learning-based model capable of classifying individuals into distinct weight categories based on demographic, lifestyle, and physiological data. The approach integrates feature engineering, data preprocessing, and ensemble learning methods to achieve high predictive accuracy. Body Mass Index (BMI) was engineered as a key feature to improve model interpretability and performance. Multiple models were compared, and the best configuration was selected through systematic hyperparameter tuning. The results demonstrate the significance of feature engineering and model optimization in improving classification accuracy.

The preprocessing pipeline was crucial for handling the mixed data types present in the dataset. \textbf{Categorical features} were handled using appropriate encoding techniques (OrdinalEncoder and OneHotEncoder) to convert them into a numerical format suitable for machine learning algorithms. Furthermore, all \textbf{numerical features} were standardized using \verb|StandardScaler| to ensure the models were not unduly influenced by differences in feature scale. This foundational preprocessing work ensured that the subsequent ensemble methods, particularly the high-performing \textbf{XGBoost Classifier}, could operate efficiently and robustly. The entire workflow, from BMI calculation to final model prediction, was managed within a structured scikit-learn pipeline for deployment readiness and reproducibility.

% ---------------------------------------------------
\chapter{Introduction}
The increasing prevalence of obesity and weight-related health conditions necessitates computational models capable of understanding and predicting weight categories from behavioral and physiological indicators. The objective of this study is to design and implement a machine learning pipeline that classifies individuals into weight categories using structured data features such as age, activity level, and dietary habits.

This report outlines the data preprocessing steps, feature engineering techniques, model training strategies, and performance evaluation metrics used. The overall aim is to achieve a high-performing and interpretable model suitable for real-world applications.

The methodology centers on integrating robust feature engineering—specifically calculating the \textbf{Body Mass Index (BMI)}—with advanced \textbf{ensemble learning techniques}. The pipeline ensures that mixed data types, including categorical and numerical features, are handled appropriately through standardized encoding and scaling prior to model ingestion. Multiple XGBoost and Random Forest configurations are systematically compared and optimized via Grid Search to maximize predictive capacity while maintaining model stability and generalizability across diverse data subsets. The final section discusses the comparative performance of the models, highlighting the significance of feature transformation and hyperparameter tuning in realizing the full potential of the classification system.

% ---------------------------------------------------
\chapter{Dataset Overview}
The dataset contains both numerical and categorical features describing participants' lifestyle, dietary patterns, and demographic characteristics.

\section{Data Description}
\begin{itemize}
    \item \textbf{Number of samples:} 15533
    \item \textbf{Number of features:} 18
    \item \textbf{Target variable:} WeightCategory (multi-class)
    \item \textbf{Feature types:} Numerical (e.g., Age, Height, Weight), Categorical (e.g., Gender, SMOKE, FAVC, MTRANS)
\end{itemize}

\section{Missing Values and Cleaning}
Basic data cleaning included removal of missing or inconsistent entries, conversion of categorical data into consistent formats, and type normalization across all columns.

% ---------------------------------------------------
\chapter{Data Preprocessing and Feature Engineering}

\section{Feature Engineering}
The most critical feature engineering step was calculating the Body Mass Index (BMI) using the formula:
\[
    \text{BMI} = \frac{\text{Weight (kg)}}{(\text{Height (m)})^2}
\]
BMI was introduced as a key feature to enhance the model’s understanding of body composition. Original columns \textit{Weight}, \textit{Height}, and \textit{id} were dropped to prevent redundancy and data leakage. This computation was automated using a custom transformer integrated into the pipeline.

\section{Encoding and Scaling}
A preprocessing pipeline was constructed using \texttt{ColumnTransformer} to handle heterogeneous data:
\begin{itemize}
    \item \textbf{Binary/Ordinal Features:} Categorical features such as Gender, SMOKE, FAVC, FCVC were encoded using \texttt{OrdinalEncoder} preserving inherent order.
    \item \textbf{Nominal Features:} MTRANS (mode of transportation) was encoded using \texttt{OneHotEncoder} with \texttt{drop='first'}.
    \item \textbf{Numerical Features:} Continuous variables like Age, NCP, BMI were standardized using \texttt{StandardScaler}.
\end{itemize}

\section{Pipeline Integration}
The preprocessing steps were integrated into a complete machine learning pipeline, ensuring consistency across training and test datasets. This design supports reproducibility and prevents data leakage during evaluation.

This robust pipeline guarantees that all steps, including the custom BMI feature calculation and the \verb|StandardScaler| application, are fitted \textit{only} on the training data and then applied to the test data. This methodology prevents the test set's statistics from influencing the training phase. By managing the entire workflow via the pipeline construct, the final optimized model (XGBoost) can be reliably saved and deployed as a single unit, ready to classify individuals into distinct weight categories based on their structured inputs for real-world applications.

% ---------------------------------------------------
\chapter{Model Development}

\section{Overview}
Four models were developed and evaluated sequentially:
\begin{enumerate}
    \item Model 1: Random Forest Classifier
    \item Model 2: Base XGBoost Classifier
    \item Model 3: Tuned XGBoost Classifier via GridSearchCV
    \item Model 4: Tuned XGBoost
\end{enumerate}

\section{Model 1: Random Forest Classifier}
\begin{itemize}
    \item \textbf{Configuration:} n\_estimators=200, random\_state=42
    \item \textbf{Performance:} Accuracy = 88.53\%
\end{itemize}

\section{Model 2: Base XGBoost Classifier}
\begin{itemize}
    \item \textbf{Configuration:} n\_estimators=100, max\_depth=6, learning\_rate=0.1
    \item \textbf{Performance:} Accuracy = 87\%
\end{itemize}

\section{Model 3: Tuned XGBoost via Grid Search}
\begin{itemize}
    \item \textbf{Methodology:} GridSearchCV with 5-fold cross-validation
    \item \textbf{Optimized Parameters:} 
    \begin{itemize}
        \item max\_depth: [4, 6, 8]
        \item learning\_rate: [0.01, 0.05, 0.1]
        \item n\_estimators: [100, 200]
        \item subsample: [0.7, 0.9, 1.0]
        \item colsample\_bytree: [0.7, 1.0]
    \end{itemize}
    \item \textbf{Performance:} Accuracy = 88.74\%
\end{itemize}

\section{Model 4: Tuned XGBoost}
\begin{itemize}
    \item \textbf{Algorithm:} XGBClassifier
    \item \textbf{Preprocessing:} OneHotEncoder for all categorical features and StandardScaler for numerical features (including BMI)
    \item \textbf{Tuning Method:} GridSearchCV with 5-fold cross-validation
    \item \textbf{Best Parameters Found:} \{learning\_rate=0.1, max\_depth=5, n\_estimators=200, subsample=1.0\}
    \item \textbf{Performance (Best CV Accuracy):} 91.074\%
    \item \textbf{Key Feature Importances:} BMI (27.1\%), Gender\_Female (25.9\%), Weight (10.1\%)
\end{itemize}

\noindent\textbf{Project Repository Links:}
\begin{itemize}
    \item \href{https://github.com/dharani070707/Obesity_classification_ML_Project}{GitHub Repository – Dharani's Project}
    \item \href{https://github.com/abhishek-prasanna11/MLProject}{GitHub Repository – Abhishek's Project}
\end{itemize}


% ---------------------------------------------------
\chapter{Results and Performance Evaluation}

\begin{table}[H]
\centering
\caption{Model Performance Comparison}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Algorithm} & \textbf{Accuracy (\%)} \\
\midrule
Model 1 & Random Forest & 88.53 \\
Model 2 & Base XGBoost & 87.00 \\
Model 3 & Tuned XGBoost & 88.74 \\
Model 4 & Tuned XGBoost & 91.074 \\
\bottomrule
\end{tabular}
\end{table}

% ---------------------------------------------------
\chapter{Discussion}
The tuned XGBoost models demonstrated superior accuracy due to optimized hyperparameters controlling learning rate, tree depth, and regularization. Random Forest provided a strong baseline with stable generalization, while the base XGBoost showed faster convergence but moderate overfitting.

Feature importance analysis revealed that BMI, Age, and food consumption frequency (FCVC) were key predictors for Models 1–3. For Model 4, BMI, Gender\_Female, and Weight were the most influential features, highlighting the role of consistent OneHotEncoding for categorical features along with BMI engineering.

\begin{itemize}
    \item \textbf{Random Forest vs Base XGBoost:} Random Forest (88.53\%) outperformed base XGBoost (87\%), showing its robustness as a baseline.
    \item \textbf{Effectiveness of Hyperparameter Tuning:} Optimized XGBoost (88.74\%) improved over the base model, demonstrating the significance of tuning for bias-variance tradeoff.
    \item \textbf{Impact of Preprocessing (Model 4):} Model 4 achieved the highest accuracy (91.074\%), illustrating that feature engineering and consistent preprocessing enhances predictive performance.
\end{itemize}

% ---------------------------------------------------
\end{document}
